![image](https://github.com/nikhilkanta2303/Assistant_Using_Ollama_featuring_mistral-llama3/assets/111275048/c2f1c147-d810-451e-9b2b-5a8dbd5795ca)


 **Local LLM Assistant with Mistral Model - A On-Premises AI Solution**

Description:

The Local Language Learning Model (LLM) Assistant is an innovative, on-premises implementation of an AI assistant that uses the On-Premises Llama (OLLAMA) engine without relying on third-party APIs. This project offers two main functionalities to interact with the LLM model:

**1. PushToRaiseApp** - Equipped with a Call-to-Action (CTA) button, this functionality prompts the user-selected LLM model to generate responses in the terminal and read them aloud, enabling seamless interaction between humans and AI.
**2. ConversationMode** - Users can set up the program as a startup application on Windows, allowing it to run automatically whenever the 'Pheonix' keyword is detected. This ensures that the assistant responds promptly when needed. Mac users can manually launch the program due to limitations in similar features for macOS.

Pre-requisites:
1. OLLAMA installed with models like llama3, mistral, llava.
2. Python programming language.
3. Integrated Development Environment (IDE) - Visual Studio Code (VS Code).
4. Imported packages required for the project's functionality.

Model: Mistral, a high-performance model offering faster computing times and improved processing efficiency by utilizing On-Premises Llama (OLLAMA). This allows for an efficient and responsive interaction with the AI assistant.

Language: Python programming language.
Preferred IDE: Visual Studio Code (VS Code).
